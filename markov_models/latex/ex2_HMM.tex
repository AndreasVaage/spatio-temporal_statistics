\documentclass[10pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{xcolor}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\author{Alberto, Andreas, Dennis}
\title{Hidden Markov Models}
\begin{document}
\maketitle

\section{Forward Recursion}
Assuming first order markov chain model with $n$ discrete variables i.e. $x_i \in \mathrm{D} \quad i = 1,\dots,n$, where $\mathrm{D}$ is discrete set of size $d$, conditionally independent measurments $y_1,\dots,y_n$, with likelihood $p(y_i|x_i)$, stationarry conditional probabilities $p(x_{i+1 = l}|x_i=k) = \mathrm{P(k,l)}$ and a intial prior for the first variable, $p(x_1)$.  %Cant remember how to mathematically notate a discrete set.
Forward recursion effectively,$\mathcal{O}(nd^2)$, estimates the posteriory estimation for the last variable $p(x_n|y_1,\dots,y_n)$.
The recursive formula is splitted into two steps, the prediction step:
\begin{equation}\label{eq:predicted}
p(x_i|y_1,\dots,y_{i-1}) = \sum_{x_{i-1} \in \mathrm{D}}p(x_i|x_{i-1},{\color{red}y_{1},\dots,y_{i-1}})p(x_{i-1}|y_1,\dots,y_{i-1})
\end{equation}
were the dependencies, ${\color{red}y_{1},\dots,y_{i-1}}$, marked in red in the markov transition probability can be removed from the equatio, due to the first order caracteristics of the markov graph. Where for each variable the transition probability only condition on the previous variable. Next the filtering step:
\begin{equation}\label{eq:filtering}
p(x_i|y_1,\dots, y_i) = \frac{p(y_i|x_i,{\color{red}y_{1},\dots,y_{i-1}})p(x_i|y_1,\dots,y_{i-1})}{p(y_i|y_1,\dots,y_{i-1})}
\end{equation}
\begin{equation}
p(y_i|y_1,\dots,y_{i-1}) = \sum_{x_i\in\mathrm{D}} p(y_i|x_i,{\color{red}y_{1},\dots,y_{i-1}}) p(x_i|y_1,\dots,y_{i-1})
\end{equation}
The dependencies, ${\color{red}y_{1},\dots,y_{i-1}}$, marked in red in the likelihood can be removed from the equations due to the conditional independence of the measurments.  
This is similar to the state estimation methods such as the Kalman filter, where the goal is to estimate the current state $x_k$, while new measurments arrive continueasly.
We are interested in finding the underlying probability parameters $\theta$ for this we want to calcualte the marginal likelihood $p(y_1,\dots,y_n)$.
This can be done during the forward iteration descrived above by calcultaing the current marginal after each filtering step as follow: 
\begin{equation}
p(y_1,\dots,y_i) = p(y_i|y_1,\dots,y_{i-1})p(y_1,\dots,y_{i-1})
\end{equation}

\subsection{Implementation}
\subsection{Results}
The marginal likelihood were calcluated for a grid of values for $p$ and $\tau^2$ based on the data $\mathbf{y}$ generated in task a.
\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{figures/forward_recursion_local_ml.eps}
    \caption{Marginal likelihood as a function of the model parameters $\theta = (\tau,p)$.}
    \label{fig:forward_recursion_local_ml}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{figures/forward_recursion_global_ml.eps}
    \caption{Marginal likelihood as a function of the model parameters $\theta = (\tau,p)$, in the local area of the correct parameters.}
    \label{fig:forward_recursion_global_ml}
\end{figure}








\end{document}